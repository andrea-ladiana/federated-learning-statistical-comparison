{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2504d7",
   "metadata": {},
   "source": [
    "# üöÄ Comprehensive Federated Learning Experiments with `run_extensive_experiments.py`\n",
    "\n",
    "This notebook provides a comprehensive guide to using the powerful `run_extensive_experiments.py` script, which is the core tool for conducting large-scale federated learning research in this framework.\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "- How to configure and run extensive experiments\n",
    "- Understanding the experiment matrix (strategies √ó attacks √ó datasets)\n",
    "- Working with the enhanced experiment runner\n",
    "- Analyzing and visualizing comprehensive results\n",
    "- Best practices for large-scale FL research\n",
    "\n",
    "## üéØ Why Use `run_extensive_experiments.py`?\n",
    "\n",
    "This script is designed for serious federated learning research that requires:\n",
    "- **Systematic comparison** of multiple FL strategies\n",
    "- **Robustness evaluation** under various attack scenarios\n",
    "- **Statistical significance** through multiple experimental runs\n",
    "- **Checkpoint/resume** functionality for long-running experiments\n",
    "- **Parallel execution** for efficient resource utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ddb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add framework to path\n",
    "framework_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(framework_root))\n",
    "\n",
    "print(f\"üîß Framework root: {framework_root}\")\n",
    "print(f\"üìÅ Current working directory: {Path.cwd()}\")\n",
    "\n",
    "# Check if run_extensive_experiments.py exists\n",
    "extensive_script = framework_root / \"experiment_runners\" / \"run_extensive_experiments.py\"\n",
    "print(f\"‚úÖ Extensive experiments script exists: {extensive_script.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a2bee",
   "metadata": {},
   "source": [
    "## üîç Understanding the Experiment Matrix\n",
    "\n",
    "The `run_extensive_experiments.py` script automatically generates a comprehensive matrix of experiments by combining:\n",
    "\n",
    "### üß† Federated Learning Strategies (19 total)\n",
    "1. **Core Strategies**: FedAvg, FedAvgM, FedProx, FedNova, SCAFFOLD, FedAdam\n",
    "2. **Byzantine-Robust**: Krum, TrimmedMean, Bulyan\n",
    "3. **Flower Baselines**: DASHA, DepthFL, HeteroFL, FedMeta, FedPer, FjORD, FLANDERS, FedOpt\n",
    "\n",
    "### ‚öîÔ∏è Attack Scenarios (7 configurations)\n",
    "1. **none** - Clean baseline\n",
    "2. **noise** - Gaussian noise injection\n",
    "3. **missed** - Client participation failures\n",
    "4. **failure** - Random client failures\n",
    "5. **asymmetry** - Data distribution asymmetry\n",
    "6. **labelflip** - Label flipping attacks\n",
    "7. **gradflip** - Gradient flipping attacks\n",
    "\n",
    "### üìä Datasets (3 available)\n",
    "- **MNIST** - Handwritten digits (28√ó28, 10 classes)\n",
    "- **FMNIST** - Fashion items (28√ó28, 10 classes)\n",
    "- **CIFAR10** - Natural images (32√ó32√ó3, 10 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aaa110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the total number of experiments\n",
    "strategies = ['fedavg', 'fedavgm', 'fedprox', 'fednova', 'scaffold', 'fedadam', \n",
    "              'krum', 'trimmedmean', 'bulyan', 'dasha', 'depthfl', 'heterofl', \n",
    "              'fedmeta', 'fedper', 'fjord', 'flanders', 'fedopt']\n",
    "\n",
    "attacks = ['none', 'noise', 'missed', 'failure', 'asymmetry', 'labelflip', 'gradflip']\n",
    "\n",
    "datasets = ['MNIST', 'FMNIST', 'CIFAR10']\n",
    "\n",
    "total_configs = len(strategies) * len(attacks) * len(datasets)\n",
    "\n",
    "print(f\"üßÆ Experiment Matrix Calculation:\")\n",
    "print(f\"   Strategies: {len(strategies)}\")\n",
    "print(f\"   Attacks: {len(attacks)}\")\n",
    "print(f\"   Datasets: {len(datasets)}\")\n",
    "print(f\"   Total configurations: {total_configs}\")\n",
    "print(f\"\\nüìà With multiple runs (e.g., 10 runs per config):\")\n",
    "print(f\"   Total experiments: {total_configs * 10:,}\")\n",
    "print(f\"   Estimated time (5 min/experiment): {(total_configs * 10 * 5) / 60:.1f} hours\")\n",
    "\n",
    "# Display some example combinations\n",
    "print(f\"\\nüéØ Example experiment combinations:\")\n",
    "examples = [\n",
    "    ('fedavg', 'none', 'MNIST', 'Baseline performance'),\n",
    "    ('fedprox', 'noise', 'CIFAR10', 'Robust strategy under noise'),\n",
    "    ('krum', 'labelflip', 'FMNIST', 'Byzantine-robust vs malicious attack'),\n",
    "    ('scaffold', 'asymmetry', 'MNIST', 'Advanced strategy vs data heterogeneity')\n",
    "]\n",
    "\n",
    "for strategy, attack, dataset, description in examples:\n",
    "    print(f\"   {strategy} + {attack} + {dataset}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c8502",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Running Your First Extensive Experiment\n",
    "\n",
    "Let's start with a small-scale test to understand how the script works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bedfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extensive_experiments_test():\n",
    "    \"\"\"Run a small test version of extensive experiments.\"\"\"\n",
    "    \n",
    "    # Change to framework directory\n",
    "    original_cwd = os.getcwd()\n",
    "    os.chdir(framework_root)\n",
    "    \n",
    "    try:\n",
    "        # Build command for test run\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"experiment_runners/run_extensive_experiments.py\",\n",
    "            \"--num-runs\", \"1\",  # Just 1 run for testing\n",
    "            \"--test-mode\",       # Enable test mode (fewer configurations)\n",
    "            \"--timeout\", \"300\"   # 5 minute timeout per experiment\n",
    "        ]\n",
    "        \n",
    "        print(f\"üöÄ Running test command: {' '.join(cmd)}\")\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üß™ Starting Test Extensive Experiments...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Execute with real-time output\n",
    "        process = subprocess.Popen(\n",
    "            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "            text=True, bufsize=1, universal_newlines=True\n",
    "        )\n",
    "        \n",
    "        # Print output in real-time\n",
    "        for line in process.stdout:\n",
    "            print(line.rstrip())\n",
    "        \n",
    "        process.wait()\n",
    "        return_code = process.returncode\n",
    "        \n",
    "        print(f\"\\n‚úÖ Test completed with return code: {return_code}\")\n",
    "        return return_code == 0\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚è∞ Test timed out - this is normal for demonstration\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running test: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        os.chdir(original_cwd)\n",
    "\n",
    "# Uncomment the next line to run the test (may take several minutes)\n",
    "# run_extensive_experiments_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1c456",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Advanced Configuration Options\n",
    "\n",
    "The `run_extensive_experiments.py` script supports many advanced options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da936b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the command-line options\n",
    "def show_extensive_experiments_help():\n",
    "    \"\"\"Display help for the extensive experiments script.\"\"\"\n",
    "    \n",
    "    original_cwd = os.getcwd()\n",
    "    os.chdir(framework_root)\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            sys.executable,\n",
    "            \"experiment_runners/run_extensive_experiments.py\",\n",
    "            \"--help\"\n",
    "        ], capture_output=True, text=True)\n",
    "        \n",
    "        print(\"üìã Available command-line options:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(result.stdout)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting help: {e}\")\n",
    "    finally:\n",
    "        os.chdir(original_cwd)\n",
    "\n",
    "show_extensive_experiments_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7b7016",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Custom Configuration Examples\n",
    "\n",
    "Here are some practical examples of how to run extensive experiments with different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configurations for different research scenarios\n",
    "\n",
    "def create_experiment_commands():\n",
    "    \"\"\"Create example commands for different research scenarios.\"\"\"\n",
    "    \n",
    "    commands = {\n",
    "        \"Quick Test\": [\n",
    "            \"python experiment_runners/run_extensive_experiments.py\",\n",
    "            \"--num-runs 1\",\n",
    "            \"--test-mode\",\n",
    "            \"--timeout 300\"\n",
    "        ],\n",
    "        \n",
    "        \"Statistical Analysis\": [\n",
    "            \"python experiment_runners/run_extensive_experiments.py\",\n",
    "            \"--num-runs 10\",\n",
    "            \"--parallel\",\n",
    "            \"--max-workers 4\"\n",
    "        ],\n",
    "        \n",
    "        \"Byzantine Robustness Study\": [\n",
    "            \"python experiment_runners/run_extensive_experiments.py\",\n",
    "            \"--num-runs 5\",\n",
    "            \"--strategies krum,trimmedmean,bulyan,fedavg\",\n",
    "            \"--attacks labelflip,gradflip,none\",\n",
    "            \"--datasets MNIST,CIFAR10\"\n",
    "        ],\n",
    "        \n",
    "        \"Communication Efficiency\": [\n",
    "            \"python experiment_runners/run_extensive_experiments.py\",\n",
    "            \"--num-runs 3\",\n",
    "            \"--strategies fedavg,fednova,scaffold\",\n",
    "            \"--attacks none,asymmetry\",\n",
    "            \"--rounds 20\",\n",
    "            \"--clients 20\"\n",
    "        ],\n",
    "        \n",
    "        \"Resume from Checkpoint\": [\n",
    "            \"python experiment_runners/run_extensive_experiments.py\",\n",
    "            \"--resume\",\n",
    "            \"--checkpoint-dir enhanced_experiment_results/checkpoints\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"üéØ Example Experiment Commands:\\n\")\n",
    "    \n",
    "    for scenario, cmd_parts in commands.items():\n",
    "        print(f\"**{scenario}:**\")\n",
    "        print(f\"```bash\")\n",
    "        print(\" \".join(cmd_parts))\n",
    "        print(f\"```\\n\")\n",
    "        \n",
    "    return commands\n",
    "\n",
    "example_commands = create_experiment_commands()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200b459",
   "metadata": {},
   "source": [
    "## üìä Understanding the Output Structure\n",
    "\n",
    "The extensive experiments generate comprehensive results in a structured format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a0f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the expected output structure\n",
    "def show_expected_output_structure():\n",
    "    \"\"\"Display the expected output structure from extensive experiments.\"\"\"\n",
    "    \n",
    "    print(\"üìÅ Expected Output Directory Structure:\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\"\"\n",
    "enhanced_experiment_results/\n",
    "‚îú‚îÄ‚îÄ final_results_YYYYMMDD_HHMMSS.csv        # Main results file\n",
    "‚îú‚îÄ‚îÄ final_results_YYYYMMDD_HHMMSS.json       # Backup in JSON format\n",
    "‚îú‚îÄ‚îÄ final_results_YYYYMMDD_HHMMSS.csv.gz     # Compressed backup\n",
    "‚îú‚îÄ‚îÄ intermediate_results_*.csv                # Periodic saves\n",
    "‚îú‚îÄ‚îÄ enhanced_experiment_runner.log            # Detailed execution log\n",
    "‚îî‚îÄ‚îÄ checkpoints/\n",
    "    ‚îú‚îÄ‚îÄ checkpoint_YYYYMMDD_HHMMSS.yaml      # Experiment state\n",
    "    ‚îî‚îÄ‚îÄ experiments_status.json              # Progress tracking\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nüìä CSV Results Columns:\")\n",
    "    print(\"=\"*30)\n",
    "    columns = [\n",
    "        ('algorithm', 'Federated learning strategy used'),\n",
    "        ('attack', 'Attack type applied (includes parameters)'),\n",
    "        ('dataset', 'Dataset used for training'),\n",
    "        ('run', 'Run number (for statistical analysis)'),\n",
    "        ('client_id', 'Client identifier (-1 for server metrics)'),\n",
    "        ('round', 'Federated learning round number'),\n",
    "        ('metric', 'Type of metric (loss, accuracy, precision, etc.)'),\n",
    "        ('value', 'Actual metric value')\n",
    "    ]\n",
    "    \n",
    "    for col, desc in columns:\n",
    "        print(f\"  {col:<12}: {desc}\")\n",
    "\n",
    "show_expected_output_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc970f3",
   "metadata": {},
   "source": [
    "## üìà Analyzing Extensive Experiment Results\n",
    "\n",
    "Once you have results, here's how to analyze them effectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_extensive_results(results_file=None):\n",
    "    \"\"\"Analyze results from extensive experiments.\"\"\"\n",
    "    \n",
    "    # If no specific file provided, look for the most recent one\n",
    "    if results_file is None:\n",
    "        results_dir = framework_root / \"enhanced_experiment_results\"\n",
    "        if results_dir.exists():\n",
    "            csv_files = list(results_dir.glob(\"final_results_*.csv\"))\n",
    "            if csv_files:\n",
    "                results_file = max(csv_files, key=lambda x: x.stat().st_mtime)\n",
    "                print(f\"üìä Using most recent results file: {results_file.name}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No results files found. Run experiments first.\")\n",
    "                return create_sample_results_for_demo()\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Results directory not found. Creating sample data for demonstration.\")\n",
    "            return create_sample_results_for_demo()\n",
    "    \n",
    "    try:\n",
    "        # Load the results\n",
    "        df = pd.read_csv(results_file)\n",
    "        print(f\"‚úÖ Loaded {len(df)} records from {results_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading results: {e}\")\n",
    "        return create_sample_results_for_demo()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_sample_results_for_demo():\n",
    "    \"\"\"Create sample results data for demonstration purposes.\"\"\"\n",
    "    \n",
    "    print(\"üé≠ Creating sample results for demonstration...\")\n",
    "    \n",
    "    # Generate realistic sample data\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    strategies = ['fedavg', 'fedprox', 'krum', 'scaffold']\n",
    "    attacks = ['none', 'noise', 'labelflip']\n",
    "    datasets = ['MNIST', 'CIFAR10']\n",
    "    metrics = ['accuracy', 'loss']\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        for attack in attacks:\n",
    "            for dataset in datasets:\n",
    "                for run in range(3):  # 3 runs per configuration\n",
    "                    for round_num in range(1, 11):  # 10 rounds\n",
    "                        # Simulate realistic accuracy progression\n",
    "                        base_acc = 0.7 if dataset == 'MNIST' else 0.6\n",
    "                        if attack == 'labelflip':\n",
    "                            base_acc *= 0.8  # Attack degrades performance\n",
    "                        if strategy == 'krum' and attack != 'none':\n",
    "                            base_acc *= 1.1  # Robust strategy helps under attack\n",
    "                        \n",
    "                        final_acc = base_acc + (round_num * 0.03) + np.random.normal(0, 0.02)\n",
    "                        final_acc = max(0.1, min(0.95, final_acc))  # Clamp to realistic range\n",
    "                        \n",
    "                        data.append({\n",
    "                            'algorithm': strategy,\n",
    "                            'attack': attack,\n",
    "                            'dataset': dataset,\n",
    "                            'run': run,\n",
    "                            'client_id': -1,  # Server metric\n",
    "                            'round': round_num,\n",
    "                            'metric': 'accuracy',\n",
    "                            'value': final_acc\n",
    "                        })\n",
    "                        \n",
    "                        # Add corresponding loss\n",
    "                        loss = 2.0 - (final_acc * 1.8) + np.random.normal(0, 0.1)\n",
    "                        loss = max(0.1, loss)\n",
    "                        \n",
    "                        data.append({\n",
    "                            'algorithm': strategy,\n",
    "                            'attack': attack,\n",
    "                            'dataset': dataset,\n",
    "                            'run': run,\n",
    "                            'client_id': -1,\n",
    "                            'round': round_num,\n",
    "                            'metric': 'loss',\n",
    "                            'value': loss\n",
    "                        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"‚úÖ Generated {len(df)} sample records\")\n",
    "    return df\n",
    "\n",
    "# Load or create sample results\n",
    "results_df = analyze_extensive_results()\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"  Total records: {len(results_df):,}\")\n",
    "print(f\"  Unique strategies: {results_df['algorithm'].nunique()}\")\n",
    "print(f\"  Unique attacks: {results_df['attack'].nunique()}\")\n",
    "print(f\"  Unique datasets: {results_df['dataset'].nunique()}\")\n",
    "print(f\"  Metrics tracked: {results_df['metric'].nunique()}\")\n",
    "print(f\"  Total runs: {results_df['run'].nunique()}\")\n",
    "\n",
    "# Show data structure\n",
    "print(f\"\\nüìã Sample Data:\")\n",
    "print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd724d",
   "metadata": {},
   "source": [
    "## üìä Comprehensive Results Visualization\n",
    "\n",
    "Let's create comprehensive visualizations to understand the experimental results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f89ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_visualizations(df):\n",
    "    \"\"\"Create comprehensive visualizations of the experimental results.\"\"\"\n",
    "    \n",
    "    # Set up the plotting environment\n",
    "    plt.rcParams['figure.figsize'] = (15, 10)\n",
    "    \n",
    "    # 1. Strategy Performance Comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    fig.suptitle('üöÄ Comprehensive Federated Learning Experiment Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Filter for final round accuracy\n",
    "    final_acc = df[(df['metric'] == 'accuracy') & (df['round'] == df['round'].max())]\n",
    "    \n",
    "    # Plot 1: Strategy performance across attacks\n",
    "    strategy_performance = final_acc.groupby(['algorithm', 'attack'])['value'].mean().unstack()\n",
    "    strategy_performance.plot(kind='bar', ax=axes[0,0], width=0.8)\n",
    "    axes[0,0].set_title('Strategy Performance Under Different Attacks')\n",
    "    axes[0,0].set_ylabel('Final Accuracy')\n",
    "    axes[0,0].legend(title='Attack Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Dataset-specific performance\n",
    "    dataset_performance = final_acc.groupby(['dataset', 'algorithm'])['value'].mean().unstack()\n",
    "    dataset_performance.plot(kind='bar', ax=axes[0,1], width=0.8)\n",
    "    axes[0,1].set_title('Strategy Performance by Dataset')\n",
    "    axes[0,1].set_ylabel('Final Accuracy')\n",
    "    axes[0,1].legend(title='Strategy', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0,1].tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # Plot 3: Learning curves for top strategies\n",
    "    acc_data = df[df['metric'] == 'accuracy']\n",
    "    top_strategies = final_acc.groupby('algorithm')['value'].mean().nlargest(4).index\n",
    "    \n",
    "    for strategy in top_strategies:\n",
    "        strategy_data = acc_data[(acc_data['algorithm'] == strategy) & (acc_data['attack'] == 'none')]\n",
    "        learning_curve = strategy_data.groupby('round')['value'].mean()\n",
    "        axes[1,0].plot(learning_curve.index, learning_curve.values, marker='o', label=strategy, linewidth=2)\n",
    "    \n",
    "    axes[1,0].set_title('Learning Curves (No Attack Scenario)')\n",
    "    axes[1,0].set_xlabel('Federated Round')\n",
    "    axes[1,0].set_ylabel('Accuracy')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Robustness analysis (performance degradation under attacks)\n",
    "    baseline_perf = final_acc[final_acc['attack'] == 'none'].groupby('algorithm')['value'].mean()\n",
    "    attack_perf = final_acc[final_acc['attack'] != 'none'].groupby(['algorithm', 'attack'])['value'].mean()\n",
    "    \n",
    "    robustness_data = []\n",
    "    for strategy in baseline_perf.index:\n",
    "        baseline = baseline_perf[strategy]\n",
    "        for attack in final_acc['attack'].unique():\n",
    "            if attack != 'none' and (strategy, attack) in attack_perf.index:\n",
    "                degradation = baseline - attack_perf[(strategy, attack)]\n",
    "                robustness_data.append({\n",
    "                    'Strategy': strategy,\n",
    "                    'Attack': attack,\n",
    "                    'Performance Degradation': degradation\n",
    "                })\n",
    "    \n",
    "    robustness_df = pd.DataFrame(robustness_data)\n",
    "    if not robustness_df.empty:\n",
    "        robustness_pivot = robustness_df.pivot(index='Strategy', columns='Attack', values='Performance Degradation')\n",
    "        sns.heatmap(robustness_pivot, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=axes[1,1], cbar_kws={'label': 'Performance Degradation'})\n",
    "        axes[1,1].set_title('Strategy Robustness Analysis\\n(Lower values = more robust)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return strategy_performance, dataset_performance, robustness_df\n",
    "\n",
    "# Create visualizations\n",
    "strategy_perf, dataset_perf, robustness_data = create_comprehensive_visualizations(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8266cff4",
   "metadata": {},
   "source": [
    "## üìà Statistical Analysis and Insights\n",
    "\n",
    "Let's perform detailed statistical analysis of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_analysis(df):\n",
    "    \"\"\"Perform comprehensive statistical analysis of the experimental results.\"\"\"\n",
    "    \n",
    "    print(\"üìä STATISTICAL ANALYSIS REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Overall Performance Summary\n",
    "    final_acc = df[(df['metric'] == 'accuracy') & (df['round'] == df['round'].max())]\n",
    "    \n",
    "    print(\"\\nüéØ OVERALL PERFORMANCE SUMMARY\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Best performing strategies\n",
    "    strategy_means = final_acc.groupby('algorithm')['value'].agg(['mean', 'std', 'count'])\n",
    "    strategy_means = strategy_means.sort_values('mean', ascending=False)\n",
    "    \n",
    "    print(\"\\nüèÜ Top 5 Strategies (by mean accuracy):\")\n",
    "    for i, (strategy, row) in enumerate(strategy_means.head().iterrows(), 1):\n",
    "        print(f\"  {i}. {strategy:<12}: {row['mean']:.3f} ¬± {row['std']:.3f} (n={row['count']})\")\n",
    "    \n",
    "    # 2. Attack Impact Analysis\n",
    "    print(\"\\n\\n‚öîÔ∏è ATTACK IMPACT ANALYSIS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    attack_impact = final_acc.groupby('attack')['value'].agg(['mean', 'std', 'count'])\n",
    "    attack_impact = attack_impact.sort_values('mean', ascending=False)\n",
    "    \n",
    "    print(\"\\nAttack severity ranking (higher accuracy = less severe):\")\n",
    "    for i, (attack, row) in enumerate(attack_impact.iterrows(), 1):\n",
    "        print(f\"  {i}. {attack:<12}: {row['mean']:.3f} ¬± {row['std']:.3f} (n={row['count']})\")\n",
    "    \n",
    "    # 3. Dataset Difficulty Analysis\n",
    "    print(\"\\n\\nüìä DATASET DIFFICULTY ANALYSIS\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    dataset_difficulty = final_acc.groupby('dataset')['value'].agg(['mean', 'std', 'count'])\n",
    "    dataset_difficulty = dataset_difficulty.sort_values('mean', ascending=False)\n",
    "    \n",
    "    print(\"\\nDataset difficulty ranking (higher accuracy = easier):\")\n",
    "    for i, (dataset, row) in enumerate(dataset_difficulty.iterrows(), 1):\n",
    "        print(f\"  {i}. {dataset:<12}: {row['mean']:.3f} ¬± {row['std']:.3f} (n={row['count']})\")\n",
    "    \n",
    "    # 4. Robustness Rankings\n",
    "    print(\"\\n\\nüõ°Ô∏è ROBUSTNESS ANALYSIS\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    # Calculate robustness score for each strategy\n",
    "    baseline_scores = final_acc[final_acc['attack'] == 'none'].groupby('algorithm')['value'].mean()\n",
    "    attack_scores = final_acc[final_acc['attack'] != 'none'].groupby('algorithm')['value'].mean()\n",
    "    \n",
    "    robustness_scores = []\n",
    "    for strategy in baseline_scores.index:\n",
    "        if strategy in attack_scores.index:\n",
    "            baseline = baseline_scores[strategy]\n",
    "            under_attack = attack_scores[strategy]\n",
    "            robustness = under_attack / baseline  # Ratio of performance retention\n",
    "            robustness_scores.append((strategy, robustness, baseline, under_attack))\n",
    "    \n",
    "    robustness_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nStrategy robustness ranking (performance retention under attacks):\")\n",
    "    for i, (strategy, robustness, baseline, under_attack) in enumerate(robustness_scores, 1):\n",
    "        print(f\"  {i}. {strategy:<12}: {robustness:.1%} retention ({baseline:.3f} ‚Üí {under_attack:.3f})\")\n",
    "    \n",
    "    # 5. Efficiency Analysis (Learning Speed)\n",
    "    print(\"\\n\\n‚ö° LEARNING EFFICIENCY ANALYSIS\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Calculate how quickly each strategy reaches 80% of its final performance\n",
    "    efficiency_scores = []\n",
    "    \n",
    "    for strategy in df['algorithm'].unique():\n",
    "        strategy_data = df[(df['algorithm'] == strategy) & (df['metric'] == 'accuracy') & (df['attack'] == 'none')]\n",
    "        if len(strategy_data) > 0:\n",
    "            final_perf = strategy_data[strategy_data['round'] == strategy_data['round'].max()]['value'].mean()\n",
    "            target_perf = final_perf * 0.8\n",
    "            \n",
    "            # Find the round where 80% performance is reached\n",
    "            round_performance = strategy_data.groupby('round')['value'].mean()\n",
    "            rounds_to_target = None\n",
    "            \n",
    "            for round_num, perf in round_performance.items():\n",
    "                if perf >= target_perf:\n",
    "                    rounds_to_target = round_num\n",
    "                    break\n",
    "            \n",
    "            if rounds_to_target:\n",
    "                efficiency_scores.append((strategy, rounds_to_target, final_perf))\n",
    "    \n",
    "    efficiency_scores.sort(key=lambda x: x[1])  # Sort by rounds needed (fewer = more efficient)\n",
    "    \n",
    "    print(\"\\nStrategy efficiency ranking (rounds to reach 80% of final performance):\")\n",
    "    for i, (strategy, rounds, final_perf) in enumerate(efficiency_scores, 1):\n",
    "        print(f\"  {i}. {strategy:<12}: {rounds} rounds (final: {final_perf:.3f})\")\n",
    "    \n",
    "    return {\n",
    "        'strategy_performance': strategy_means,\n",
    "        'attack_impact': attack_impact,\n",
    "        'dataset_difficulty': dataset_difficulty,\n",
    "        'robustness_scores': robustness_scores,\n",
    "        'efficiency_scores': efficiency_scores\n",
    "    }\n",
    "\n",
    "# Perform statistical analysis\n",
    "analysis_results = perform_statistical_analysis(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e7a41",
   "metadata": {},
   "source": [
    "## üéØ Research Recommendations\n",
    "\n",
    "Based on the analysis, here are actionable recommendations for federated learning research:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221866c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_research_recommendations(analysis_results):\n",
    "    \"\"\"Generate actionable research recommendations based on the analysis.\"\"\"\n",
    "    \n",
    "    print(\"üéØ RESEARCH RECOMMENDATIONS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Get top performing strategies\n",
    "    top_strategies = analysis_results['strategy_performance'].head(3).index.tolist()\n",
    "    most_robust = [x[0] for x in analysis_results['robustness_scores'][:3]]\n",
    "    most_efficient = [x[0] for x in analysis_results['efficiency_scores'][:3]]\n",
    "    \n",
    "    recommendations = [\n",
    "        {\n",
    "            'category': 'üìà Performance Optimization',\n",
    "            'recommendations': [\n",
    "                f\"Consider {', '.join(top_strategies)} as primary strategies for high-accuracy scenarios\",\n",
    "                \"Implement hyperparameter tuning for top-performing strategies\",\n",
    "                \"Investigate ensemble methods combining multiple top strategies\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'category': 'üõ°Ô∏è Security & Robustness',\n",
    "            'recommendations': [\n",
    "                f\"Use {', '.join(most_robust)} for adversarial environments\",\n",
    "                \"Implement early attack detection mechanisms\",\n",
    "                \"Develop adaptive defense strategies that switch based on detected threats\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'category': '‚ö° Efficiency & Scalability',\n",
    "            'recommendations': [\n",
    "                f\"Choose {', '.join(most_efficient)} for resource-constrained scenarios\",\n",
    "                \"Implement adaptive learning rates based on convergence monitoring\",\n",
    "                \"Consider client sampling strategies to reduce communication overhead\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'category': 'üî¨ Future Research Directions',\n",
    "            'recommendations': [\n",
    "                \"Investigate personalized federated learning approaches\",\n",
    "                \"Develop privacy-preserving techniques beyond differential privacy\",\n",
    "                \"Study federated learning in edge computing environments\",\n",
    "                \"Explore cross-silo federated learning with different organizations\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'category': 'üìä Experimental Best Practices',\n",
    "            'recommendations': [\n",
    "                \"Run at least 10 repetitions for statistical significance\",\n",
    "                \"Include confidence intervals in all performance reports\",\n",
    "                \"Test multiple datasets to ensure generalizability\",\n",
    "                \"Document all hyperparameters and experimental conditions\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for rec_group in recommendations:\n",
    "        print(f\"\\n{rec_group['category']}\")\n",
    "        print(\"-\" * len(rec_group['category']))\n",
    "        for i, rec in enumerate(rec_group['recommendations'], 1):\n",
    "            print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    # Configuration recommendations\n",
    "    print(f\"\\n\\n‚öôÔ∏è CONFIGURATION RECOMMENDATIONS\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    config_recommendations = {\n",
    "        \"High-Performance Setup\": {\n",
    "            \"strategies\": top_strategies,\n",
    "            \"num_rounds\": \"15-20\",\n",
    "            \"num_clients\": \"20-50\",\n",
    "            \"use_case\": \"When accuracy is the primary concern\"\n",
    "        },\n",
    "        \"Robust Setup\": {\n",
    "            \"strategies\": most_robust,\n",
    "            \"num_rounds\": \"20-30\",\n",
    "            \"num_clients\": \"10-20\",\n",
    "            \"use_case\": \"When operating in adversarial environments\"\n",
    "        },\n",
    "        \"Efficient Setup\": {\n",
    "            \"strategies\": most_efficient,\n",
    "            \"num_rounds\": \"10-15\",\n",
    "            \"num_clients\": \"5-15\",\n",
    "            \"use_case\": \"When minimizing communication overhead is critical\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for setup_name, config in config_recommendations.items():\n",
    "        print(f\"\\n{setup_name}:\")\n",
    "        print(f\"  Strategies: {', '.join(config['strategies'])}\")\n",
    "        print(f\"  Rounds: {config['num_rounds']}\")\n",
    "        print(f\"  Clients: {config['num_clients']}\")\n",
    "        print(f\"  Use case: {config['use_case']}\")\n",
    "\n",
    "# Generate recommendations\n",
    "generate_research_recommendations(analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8539f711",
   "metadata": {},
   "source": [
    "## üöÄ Running Production Experiments\n",
    "\n",
    "Now that you understand the framework, here's how to run production-scale experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_production_experiment_script():\n",
    "    \"\"\"Create a production-ready experiment script.\"\"\"\n",
    "    \n",
    "    script_content = '''#!/bin/bash\n",
    "# Production Federated Learning Experiment Script\n",
    "# Generated by the Comprehensive FL Framework\n",
    "\n",
    "set -e  # Exit on any error\n",
    "\n",
    "echo \"üöÄ Starting Production Federated Learning Experiments\"\n",
    "echo \"=================================================\"\n",
    "\n",
    "# Configuration\n",
    "NUM_RUNS=10\n",
    "MAX_WORKERS=4\n",
    "TIMEOUT=1800  # 30 minutes per experiment\n",
    "RESULTS_DIR=\"production_results_$(date +%Y%m%d_%H%M%S)\"\n",
    "\n",
    "# Create results directory\n",
    "mkdir -p \"$RESULTS_DIR\"\n",
    "\n",
    "echo \"üìÅ Results will be saved to: $RESULTS_DIR\"\n",
    "echo \"‚öôÔ∏è Configuration: $NUM_RUNS runs, $MAX_WORKERS workers, ${TIMEOUT}s timeout\"\n",
    "\n",
    "# Run the extensive experiments\n",
    "python experiment_runners/run_extensive_experiments.py \\\n",
    "    --num-runs $NUM_RUNS \\\n",
    "    --parallel \\\n",
    "    --max-workers $MAX_WORKERS \\\n",
    "    --timeout $TIMEOUT \\\n",
    "    --results-dir \"$RESULTS_DIR\" \\\n",
    "    --config configuration/enhanced_config.yaml \\\n",
    "    --checkpoint-interval 5 \\\n",
    "    --save-intermediate \\\n",
    "    --verbose\n",
    "\n",
    "# Check if experiments completed successfully\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"‚úÖ Experiments completed successfully!\"\n",
    "    echo \"üìä Results available in: $RESULTS_DIR\"\n",
    "    \n",
    "    # Optionally, run analysis\n",
    "    echo \"üîç Running basic analysis...\"\n",
    "    python scripts/analyze_results.py \"$RESULTS_DIR\"\n",
    "else\n",
    "    echo \"‚ùå Experiments failed. Check logs for details.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"üéâ Production experiment pipeline completed!\"\n",
    "'''\n",
    "    \n",
    "    script_path = framework_root / \"run_production_experiments.sh\"\n",
    "    \n",
    "    try:\n",
    "        with open(script_path, 'w') as f:\n",
    "            f.write(script_content)\n",
    "        \n",
    "        # Make script executable (Unix-like systems)\n",
    "        import stat\n",
    "        script_path.chmod(script_path.stat().st_mode | stat.S_IEXEC)\n",
    "        \n",
    "        print(f\"‚úÖ Created production experiment script: {script_path}\")\n",
    "        print(f\"\\nüöÄ To run production experiments:\")\n",
    "        print(f\"   bash {script_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating script: {e}\")\n",
    "\n",
    "# Create the production script\n",
    "create_production_experiment_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0542cbb",
   "metadata": {},
   "source": [
    "## üéì Summary and Next Steps\n",
    "\n",
    "Congratulations! You've learned how to use the comprehensive `run_extensive_experiments.py` script for federated learning research.\n",
    "\n",
    "### üåü Key Takeaways\n",
    "\n",
    "1. **Comprehensive Testing**: The script automatically tests 19 strategies √ó 7 attack types √ó 3 datasets\n",
    "2. **Robust Execution**: Built-in checkpoint/resume, parallel execution, and error handling\n",
    "3. **Rich Analysis**: Detailed results in long-form format for statistical analysis\n",
    "4. **Production Ready**: Suitable for research papers and production deployments\n",
    "\n",
    "### üî¨ Research Applications\n",
    "\n",
    "This framework is ideal for:\n",
    "- **Algorithm Comparison Studies**: Compare multiple FL strategies systematically\n",
    "- **Security Research**: Evaluate defenses against various attack types\n",
    "- **Robustness Analysis**: Test performance under different conditions\n",
    "- **Educational Purposes**: Learn FL concepts through hands-on experimentation\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "1. **Run Your Own Experiments**: Start with test mode, then scale up\n",
    "2. **Analyze Results**: Use the visualization and analysis tools provided\n",
    "3. **Customize Configurations**: Modify strategies, attacks, and parameters\n",
    "4. **Contribute**: Share your findings and improvements with the community\n",
    "\n",
    "### ü§ù Need Help?\n",
    "\n",
    "- Check the `README.md` for detailed documentation\n",
    "- Explore other example notebooks in this directory\n",
    "- Review the source code in `experiment_runners/`\n",
    "- Consult the scientific papers referenced in the documentation\n",
    "\n",
    "Happy experimenting! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
