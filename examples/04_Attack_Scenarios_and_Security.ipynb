{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5ca0fa",
   "metadata": {},
   "source": [
    "# 04 - Attack Scenarios and Security\n",
    "\n",
    "This notebook demonstrates how to test federated learning systems against various attacks to evaluate robustness and security.\n",
    "\n",
    "## What You'll Learn\n",
    "- Available attack types and their effects\n",
    "- How to configure and run security experiments\n",
    "- Evaluating model robustness against attacks\n",
    "- Best practices for security testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b7ebe",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb33459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from experiment_runners.run_with_attacks import main as run_attacks\n",
    "from attacks import (\n",
    "    noise_injection, label_flipping, gradient_flipping,\n",
    "    client_failure, data_asymmetry, missed_class\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c8ed39",
   "metadata": {},
   "source": [
    "## 2. Available Attack Types\n",
    "\n",
    "The framework supports several attack scenarios to test model robustness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available attacks and their descriptions\n",
    "attacks_info = {\n",
    "    'noise_injection': 'Adds random noise to model parameters',\n",
    "    'label_flipping': 'Flips labels in training data (data poisoning)',\n",
    "    'gradient_flipping': 'Flips gradients before aggregation',\n",
    "    'client_failure': 'Simulates client dropouts and failures',\n",
    "    'data_asymmetry': 'Creates non-IID data distribution',\n",
    "    'missed_class': 'Clients missing specific classes'\n",
    "}\n",
    "\n",
    "print(\"Available Attack Scenarios:\")\n",
    "print(\"=\" * 40)\n",
    "for attack, description in attacks_info.items():\n",
    "    print(f\"• {attack}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baab429",
   "metadata": {},
   "source": [
    "## 3. Basic Attack Configuration\n",
    "\n",
    "Configure attacks with different intensity levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example attack configurations\n",
    "attack_configs = {\n",
    "    'noise_injection': {\n",
    "        'type': 'noise_injection',\n",
    "        'intensity': 0.1,  # Noise level (0.0 to 1.0)\n",
    "        'affected_clients': 0.3  # 30% of clients affected\n",
    "    },\n",
    "    'label_flipping': {\n",
    "        'type': 'label_flipping',\n",
    "        'flip_rate': 0.2,  # 20% of labels flipped\n",
    "        'affected_clients': 0.2\n",
    "    },\n",
    "    'client_failure': {\n",
    "        'type': 'client_failure',\n",
    "        'failure_rate': 0.15,  # 15% client dropout rate\n",
    "        'random_failures': True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Attack Configuration Examples:\")\n",
    "for name, config in attack_configs.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd11ac18",
   "metadata": {},
   "source": [
    "## 4. Running Security Experiments\n",
    "\n",
    "Test model robustness against different attacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9786a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple security test function\n",
    "def run_security_test(attack_type, dataset='MNIST', strategy='FedAvg'):\n",
    "    \"\"\"\n",
    "    Run a security test with specified attack.\n",
    "    \n",
    "    Args:\n",
    "        attack_type: Type of attack to simulate\n",
    "        dataset: Dataset to use (MNIST, FashionMNIST, CIFAR10)\n",
    "        strategy: Aggregation strategy\n",
    "    \"\"\"\n",
    "    print(f\"Running security test: {attack_type} on {dataset} with {strategy}\")\n",
    "    \n",
    "    # This would typically call the actual experiment runner\n",
    "    # For demonstration, we'll simulate results\n",
    "    baseline_accuracy = np.random.uniform(0.85, 0.95)\n",
    "    attacked_accuracy = baseline_accuracy - np.random.uniform(0.05, 0.25)\n",
    "    \n",
    "    return {\n",
    "        'baseline_accuracy': baseline_accuracy,\n",
    "        'attacked_accuracy': max(0, attacked_accuracy),\n",
    "        'robustness_score': attacked_accuracy / baseline_accuracy if baseline_accuracy > 0 else 0\n",
    "    }\n",
    "\n",
    "# Test different attacks\n",
    "results = {}\n",
    "test_attacks = ['noise_injection', 'label_flipping', 'client_failure']\n",
    "\n",
    "for attack in test_attacks:\n",
    "    results[attack] = run_security_test(attack)\n",
    "    print(f\"\\n{attack.upper()} Results:\")\n",
    "    print(f\"  Baseline Accuracy: {results[attack]['baseline_accuracy']:.3f}\")\n",
    "    print(f\"  Under Attack: {results[attack]['attacked_accuracy']:.3f}\")\n",
    "    print(f\"  Robustness Score: {results[attack]['robustness_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b2a68",
   "metadata": {},
   "source": [
    "## 5. Visualizing Attack Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of attack impact\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "attacks = list(results.keys())\n",
    "baseline_accs = [results[a]['baseline_accuracy'] for a in attacks]\n",
    "attacked_accs = [results[a]['attacked_accuracy'] for a in attacks]\n",
    "\n",
    "x = np.arange(len(attacks))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, baseline_accs, width, label='Baseline', alpha=0.8)\n",
    "ax1.bar(x + width/2, attacked_accs, width, label='Under Attack', alpha=0.8)\n",
    "ax1.set_xlabel('Attack Type')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Model Performance: Baseline vs Under Attack')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([a.replace('_', ' ').title() for a in attacks], rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Robustness scores\n",
    "robustness_scores = [results[a]['robustness_score'] for a in attacks]\n",
    "colors = ['green' if s > 0.8 else 'orange' if s > 0.6 else 'red' for s in robustness_scores]\n",
    "\n",
    "ax2.bar(attacks, robustness_scores, color=colors, alpha=0.7)\n",
    "ax2.set_xlabel('Attack Type')\n",
    "ax2.set_ylabel('Robustness Score')\n",
    "ax2.set_title('Model Robustness Against Attacks')\n",
    "ax2.set_xticklabels([a.replace('_', ' ').title() for a in attacks], rotation=45)\n",
    "ax2.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='Good (>0.8)')\n",
    "ax2.axhline(y=0.6, color='orange', linestyle='--', alpha=0.5, label='Fair (>0.6)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96e0946",
   "metadata": {},
   "source": [
    "## 6. Strategy Robustness Comparison\n",
    "\n",
    "Compare how different aggregation strategies handle attacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3456934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different strategies against attacks\n",
    "strategies = ['FedAvg', 'FedProx', 'Krum', 'TrimmedMean']\n",
    "attack_type = 'noise_injection'\n",
    "\n",
    "strategy_results = {}\n",
    "for strategy in strategies:\n",
    "    strategy_results[strategy] = run_security_test(attack_type, strategy=strategy)\n",
    "\n",
    "# Create robustness comparison\n",
    "df_results = pd.DataFrame({\n",
    "    'Strategy': strategies,\n",
    "    'Baseline': [strategy_results[s]['baseline_accuracy'] for s in strategies],\n",
    "    'Under Attack': [strategy_results[s]['attacked_accuracy'] for s in strategies],\n",
    "    'Robustness': [strategy_results[s]['robustness_score'] for s in strategies]\n",
    "})\n",
    "\n",
    "print(f\"Strategy Robustness Against {attack_type.upper()}:\")\n",
    "print(\"=\" * 50)\n",
    "print(df_results.round(3).to_string(index=False))\n",
    "\n",
    "# Find most robust strategy\n",
    "most_robust = df_results.loc[df_results['Robustness'].idxmax(), 'Strategy']\n",
    "print(f\"\\nMost robust strategy: {most_robust}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88676ce1",
   "metadata": {},
   "source": [
    "## 7. Advanced Attack Scenarios\n",
    "\n",
    "Test combined attacks and adaptive scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a216e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate combined attack scenario\n",
    "def combined_attack_test():\n",
    "    \"\"\"\n",
    "    Test model against multiple simultaneous attacks.\n",
    "    \"\"\"\n",
    "    print(\"Testing Combined Attack Scenario:\")\n",
    "    print(\"- 20% clients with label flipping\")\n",
    "    print(\"- 15% clients with noise injection\")\n",
    "    print(\"- 10% client failures\")\n",
    "    \n",
    "    # Simulate combined attack impact\n",
    "    baseline = 0.90\n",
    "    # Combined attacks typically have compounding effects\n",
    "    combined_impact = 0.15 + 0.10 + 0.05  # Individual impacts don't simply add\n",
    "    attacked_accuracy = max(0.1, baseline - combined_impact)\n",
    "    \n",
    "    return {\n",
    "        'baseline': baseline,\n",
    "        'attacked': attacked_accuracy,\n",
    "        'total_impact': baseline - attacked_accuracy\n",
    "    }\n",
    "\n",
    "combined_result = combined_attack_test()\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Baseline Accuracy: {combined_result['baseline']:.3f}\")\n",
    "print(f\"Under Combined Attack: {combined_result['attacked']:.3f}\")\n",
    "print(f\"Total Impact: {combined_result['total_impact']:.3f}\")\n",
    "print(f\"Robustness: {combined_result['attacked']/combined_result['baseline']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad6f26",
   "metadata": {},
   "source": [
    "## 8. Security Best Practices\n",
    "\n",
    "Key recommendations for secure federated learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d09cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security recommendations based on attack testing\n",
    "security_practices = {\n",
    "    'Strategy Selection': [\n",
    "        'Use robust aggregation (Krum, TrimmedMean) for Byzantine tolerance',\n",
    "        'Consider FedProx for heterogeneous environments',\n",
    "        'Avoid simple FedAvg in adversarial settings'\n",
    "    ],\n",
    "    'Client Management': [\n",
    "        'Implement client authentication and validation',\n",
    "        'Monitor client behavior for anomalies',\n",
    "        'Use client sampling to reduce attack surface'\n",
    "    ],\n",
    "    'Data Protection': [\n",
    "        'Validate data quality before training',\n",
    "        'Use differential privacy when possible',\n",
    "        'Implement secure aggregation protocols'\n",
    "    ],\n",
    "    'Monitoring': [\n",
    "        'Track model performance degradation',\n",
    "        'Monitor convergence patterns',\n",
    "        'Set up alerts for suspicious behavior'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"FEDERATED LEARNING SECURITY BEST PRACTICES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for category, practices in security_practices.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"  • {practice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7033fa33",
   "metadata": {},
   "source": [
    "## 9. Running Real Security Experiments\n",
    "\n",
    "Use the framework's experiment runner for comprehensive testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffeb570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to run real security experiments\n",
    "# (Uncomment and modify paths as needed)\n",
    "\n",
    "example_config = {\n",
    "    'dataset': 'MNIST',\n",
    "    'num_clients': 10,\n",
    "    'num_rounds': 20,\n",
    "    'strategy': 'FedAvg',\n",
    "    'attacks': [\n",
    "        {\n",
    "            'type': 'noise_injection',\n",
    "            'intensity': 0.1,\n",
    "            'affected_clients': 0.3\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"To run a real security experiment, use:\")\n",
    "print(\"\")\n",
    "print(\"from experiment_runners.run_with_attacks import main\")\n",
    "print(\"\")\n",
    "print(\"# Configure your experiment\")\n",
    "print(\"config = {\")\n",
    "for key, value in example_config.items():\n",
    "    print(f\"    '{key}': {repr(value)},\")\n",
    "print(\"}\")\n",
    "print(\"\")\n",
    "print(\"# Run the experiment\")\n",
    "print(\"# results = main(config)\")\n",
    "print(\"\")\n",
    "print(\"⚠️  Note: Actual experiments may take significant time to complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974d348",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "✅ **Attack Types**: Noise injection, label flipping, gradient flipping, client failures, data asymmetry, and missed classes\n",
    "\n",
    "✅ **Security Testing**: How to configure and run experiments with different attack scenarios\n",
    "\n",
    "✅ **Robustness Evaluation**: Comparing model performance under attack vs baseline\n",
    "\n",
    "✅ **Strategy Comparison**: Testing which aggregation strategies are most robust\n",
    "\n",
    "✅ **Best Practices**: Security recommendations for production federated learning systems\n",
    "\n",
    "### Next Steps:\n",
    "- Run comprehensive security evaluations on your specific use case\n",
    "- Implement monitoring and detection systems\n",
    "- Consider advanced defense mechanisms like differential privacy\n",
    "- Test your system against adaptive and sophisticated attacks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
